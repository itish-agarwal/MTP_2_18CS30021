# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13JbLnRAuSiSME2xPlIkyhf8b5ZxueblS
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
# import data processing and visualisation libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# import image processing libraries
import cv2
import skimage
from skimage.transform import resize
from sklearn.model_selection import train_test_split

# import tensorflow and keras
import tensorflow as tf
from tensorflow import keras
import os

print("Packages imported...")

from google.colab import drive
drive.mount('/content/drive')







from IPython.lib.display import join
batch_size = 64
imageSize = 128
target_dims = (imageSize, imageSize, 3)
num_classes = 29

train_len = 0
train_dir = r"/content/drive/MyDrive/asl_complete/asl_alphabet_train/asl_alphabet_train"

total = 0
for folder in os.listdir(train_dir):
    for img in os.listdir(train_dir + "/" + folder):
        train_len += 1
train_len

len = 26
S = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
map = {}

for j in range(len):
  map[S[j:j+1]] = join

t_lower = 50
t_upper = 150

def get_data(folder):
    x = []
    y = []
    cnt, label = 0, 0
    for folderName in sorted(os.listdir(folder)):
        s = str(folderName)
        for j in range(len):
          if S[j] == s:
            label = j
        
        print(label, folderName)

        for image_filename in os.listdir(folder + '/' + folderName):
            image_path = folder + '/' + folderName + '/' + image_filename
            img_file = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)
            if img_file is not None:
                img_file = cv2.resize(img_file, (imageSize, imageSize))
                img_file = cv2.cvtColor(img_file, cv2.COLOR_BGR2GRAY)
                # img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))
                # img_arr = np.asarray(img_file).reshape((-1, imageSize, imageSize, 3))
                # edge = cv2.Canny(img_file, t_lower, t_upper)
                x.append(img_file)
                y.append(label)
                cnt += 1

    return x,y

X, Y = get_data(train_dir)

print("Images successfully imported...")

X = np.array(X)
Y = np.array(Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y)


print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)



print("The shape of X_train is : ", x_train.shape)
print("The shape of y_train is : ", y_train.shape)

print("The shape of one image is : ", x_train[0].shape)

plt.imshow(x_train[0])
plt.show()

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3,random_state=42,stratify=Y_data)

from tensorflow.keras.utils import to_categorical
NUM_CLASSES = 26

y_train = to_categorical(y_train, NUM_CLASSES)
y_test = to_categorical(y_test, NUM_CLASSES)
y_train[0]

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#MODEL

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten, Dropout
print("Packages imported...")

model = Sequential()

# Convolutional Layer 1
model.add(Conv2D(64, kernel_size = 4, padding = 'same', input_shape = (imageSize, imageSize, 1)))
model.add(Activation('relu'))

# Convolutional Layer 2
model.add(Conv2D(64, kernel_size = 4, padding = 'same'))
model.add(Activation('relu'))


#Max-pooling layer
model.add(MaxPooling2D((2, 2)))

#Dropout layer
model.add(Dropout(0.5))

# Convolutional Layer 3
model.add(Conv2D(128 , kernel_size = 5, padding = 'same'))
model.add(Activation('relu'))

# Convolutional Layer 4
# model.add(Conv2D(128 , kernel_size = 5, padding = 'same'))
# model.add(Activation('relu'))

#Max-pooling layer
model.add(MaxPooling2D(pool_size = (2, 2)))

#Dropout layer
model.add(Dropout(0.5))

# Convolutional Layer 5
# model.add(Conv2D(256 , kernel_size = 5, padding = 'same'))
# model.add(Activation('relu'))

#Dropout layer
# model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(26, activation='softmax'))

# from tensorflow.keras.callbacks import EarlyStopping
# early_stop = EarlyStopping(monitor='val_loss',patience=2)

optimizer = keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1)

history = model.fit(x_train, y_train, epochs=15, batch_size=128, validation_split = 0.2)

metrics = pd.DataFrame(model.history.history)
print("The model metrics are")
metrics

metrics[['loss','val_loss']].plot()
plt.show()

metrics[['val_loss', 'accuracy', 'val_accuracy']].plot()
plt.xlabel("epoch")
plt.show()

model.evaluate(x_test, y_test)

plt.imshow(x_test[0])
plt.show()



predictions = model.predict(x_test)
print("Predictions done...")
classes = np.argmax(predictions, axis=1)

classes

#Check results on a few select images
n=np.random.randint(0, x_test.shape[0])
img = x_test[n]
plt.imshow(img)
input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)

label = 

# input_img_feature=VGG_model.predict(input_img)
# input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)
# prediction = model.predict(input_img_features)[0] 
# prediction = le.inverse_transform([prediction])  #Reverse the label encoder to original name
# print("The prediction for this image is: ", prediction)
# print("The actual label for this image is: ", test_labels[n])

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test,predictions))

plt.figure(figsize=(12,12))
sns.heatmap(confusion_matrix(y_test,predictions))
plt.show()









